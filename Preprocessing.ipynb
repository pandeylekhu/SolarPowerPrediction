{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Import the required packages</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime,timedelta\n",
    "from dateutil.relativedelta import relativedelt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Read the Label data and predictor features </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to_yymmdd = lambda dt: datetime.strptime(dt, \"%d%m%d\")\n",
    "to_mmddyyyyHHMM = lambda dt: datetime.strptime(dt, \"%m/%d/%Y %H:%M\")\n",
    "df_power=pd.read_csv('power_actual.csv',index_col=0,converters={'datetime':to_mmddyyyyHHMM})\n",
    "df_wether=pd.read_csv('weather_actuals.csv',index_col=0,converters={'datetime_utc':to_mmddyyyyHHMM,'datetime_local':to_mmddyyyyHHMM,'sunrise':to_mmddyyyyHHMM,'sunset':to_mmddyyyyHHMM,'updated_at':to_mmddyyyyHHMM})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Looking at the unique values in columns ghi and gti, we can say that these columns are either irrelevant for the data provided for these columns are incorrect.<br>\n",
    "Hence we can drop these columns</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0. nan]\n",
      "[ 0. nan]\n",
      "[0.00000e+00 8.00000e-02 3.40000e-01 ... 5.60224e+03 3.92693e+03\n",
      " 2.61160e+03]\n"
     ]
    }
   ],
   "source": [
    "print(df_power.ghi.unique())\n",
    "print(df_power.gti.unique())\n",
    "print(df_power.power.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_power.drop(['ghi','gti'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Check for NAN values</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime    0\n",
       "power       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_power.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Check for the duration within a day when solar power is generated</h3>\n",
    "<h4>Looking at the given data we can see that Power is generated on between day time i.e between 6am to 6pm</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records, power generated after  5  AM and before  5  PM: 27998\n",
      "Number of records, no power generated after  5  AM and before  5  PM: 9962\n",
      "Number of records, power generated after  6  AM and before  6  PM: 27998\n",
      "Number of records, no power generated after  6  AM and before  6  PM: 9962\n",
      "Number of records, power generated after  7  AM and before  7  PM: 26900\n",
      "Number of records, no power generated after  7  AM and before  7  PM: 11060\n",
      "Number of records, power generated after  8  AM and before  8  PM: 24624\n",
      "Number of records, no power generated after  8  AM and before  8  PM: 13336\n",
      "Number of records, power generated after  9  AM and before  9  PM: 22136\n",
      "Number of records, no power generated after  9  AM and before  9  PM: 15824\n",
      "Number of records, power generated after  10  AM and before  10  PM: 19619\n",
      "Number of records, no power generated after  10  AM and before  10  PM: 18341\n",
      "Total Number of records: 32120\n",
      "Number of records, power generated before 6 AM and after 6 PM: 0\n",
      "Number of records, no power generated before 6 AM and after 6 PM: 32120\n"
     ]
    }
   ],
   "source": [
    "for hh in range(5,11):\n",
    "    df_power2=df_power[(df_power.datetime.dt.hour>=hh) & (df_power.datetime.dt.hour<=hh+12)]\n",
    "    print('Number of records, power generated after ',hh,' AM and before ',hh,' PM:',df_power2[df_power2.power!=0].shape[0])\n",
    "    print('Number of records, no power generated after ',hh,' AM and before ',hh,' PM:',df_power2[df_power2.power==0].shape[0])\n",
    "del(df_power2)    \n",
    "df_power2=df_power[(df_power.datetime.dt.hour<6) | (df_power.datetime.dt.hour>18)]\n",
    "print('Total Number of records:',df_power2.shape[0])\n",
    "print('Number of records, power generated before 6 AM and after 6 PM:',df_power2[df_power2.power!=0].shape[0])\n",
    "print('Number of records, no power generated before 6 AM and after 6 PM:',df_power2[df_power2.power==0].shape[0])\n",
    "del(df_power2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Selected the time frame between 6AM to 6PM as out of this window there is no power generated.</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records, power generated after 6 AM and before 6 PM: 27998\n",
      "Number of records, no power generated after 6 AM and before 6 PM: 9962\n"
     ]
    }
   ],
   "source": [
    "df_power=df_power[(df_power.datetime.dt.hour>=6) & (df_power.datetime.dt.hour<=18)]\n",
    "print('Number of records, power generated after 6 AM and before 6 PM:',df_power[df_power.power!=0].shape[0])\n",
    "print('Number of records, no power generated after 6 AM and before 6 PM:',df_power[df_power.power==0].shape[0])\n",
    "df_power.sort_values('datetime',ascending=True,inplace=True)\n",
    "df_wether2=df_wether[(df_wether.datetime_local.dt.hour>=6) & (df_wether.datetime_local.dt.hour<=18)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Check for the NAN values and treat them</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "wind_chill             5349\n",
       "heat_index             5349\n",
       "qpf                    5349\n",
       "snow                   5349\n",
       "pop                    5349\n",
       "fctcode                5349\n",
       "precip_accumulation    5349\n",
       "precip_type            3723\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_wether2.isna().sum()[df_wether2.isna().sum()>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-9999.    nan]\n",
      "[-9999.    nan]\n",
      "[-9999.    nan]\n",
      "[-9999.    nan]\n",
      "[-9999.    nan]\n",
      "[-9999.    nan]\n",
      "[-9999.    nan]\n",
      "['rain' '-9999' nan]\n"
     ]
    }
   ],
   "source": [
    "print(df_wether2.wind_chill.unique())\n",
    "print(df_wether2.heat_index.unique())\n",
    "print(df_wether2.qpf.unique())\n",
    "print(df_wether2.snow.unique())\n",
    "print(df_wether2['pop'].unique())\n",
    "print(df_wether2.fctcode.unique())\n",
    "print(df_wether2.precip_accumulation.unique())\n",
    "print(df_wether2.precip_type.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Looking at the unique values in columns except for Precip_type column, we can say that these columns are either irrelevant or the data provided for these columns are incorrect.<br>\n",
    "Hence we can drop these columns and treat Precip_type column</h4>\n",
    "<h4>At the same time we will sort the data on date time</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lekhu\\AppData\\Roaming\\Python\\Python37\\site-packages\\pandas\\core\\frame.py:4117: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n",
      "C:\\Users\\Lekhu\\AppData\\Roaming\\Python\\Python37\\site-packages\\pandas\\core\\frame.py:4259: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  **kwargs\n",
      "C:\\Users\\Lekhu\\AppData\\Roaming\\Python\\Python37\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "df_wether2.drop(['wind_chill','heat_index','qpf','snow','pop','fctcode','precip_accumulation'],axis=1,inplace=True)\n",
    "df_wether2.fillna('-9999',inplace=True)\n",
    "df_wether2.sort_values('datetime_local',inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Check for data type of all the features</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 7377 entries, 6 to 13613\n",
      "Data columns (total 23 columns):\n",
      "plant_id                7377 non-null int64\n",
      "datetime_utc            7377 non-null datetime64[ns]\n",
      "datetime_local          7377 non-null datetime64[ns]\n",
      "cloud_cover             7377 non-null float64\n",
      "apparent_temperature    7377 non-null float64\n",
      "temperature             7377 non-null int64\n",
      "humidity                7377 non-null int64\n",
      "dew_point               7377 non-null float64\n",
      "wind_bearing            7377 non-null int64\n",
      "wind_speed              7377 non-null float64\n",
      "wind_gust               7377 non-null float64\n",
      "pressure                7377 non-null float64\n",
      "uv_index                7377 non-null int64\n",
      "ozone                   7377 non-null float64\n",
      "precip_intensity        7377 non-null float64\n",
      "precip_probability      7377 non-null float64\n",
      "precip_type             7377 non-null object\n",
      "visibility              7377 non-null float64\n",
      "sunrise                 7377 non-null datetime64[ns]\n",
      "sunset                  7377 non-null datetime64[ns]\n",
      "icon                    7377 non-null object\n",
      "summary                 7377 non-null object\n",
      "updated_at              7377 non-null datetime64[ns]\n",
      "dtypes: datetime64[ns](5), float64(10), int64(5), object(3)\n",
      "memory usage: 1.4+ MB\n"
     ]
    }
   ],
   "source": [
    "df_wether2.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Function to create a ordinal feature for hour of the day</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pahar(hr):\n",
    "    \"\"\" We will give more weight to afternoon hours as during this time solar heat and radiation is more.\n",
    "    Resulting in more generation of solar power\"\"\"\n",
    "    if (hr>=6 and hr<9): return(4)    #Sunrise time\n",
    "    elif hr>=9 and hr<12: return(3)   #Morning Time\n",
    "    elif hr>=12 and hr<15: return(1)  #Afternoon\n",
    "    elif hr>=15 and hr<=18: return(2) #Evening"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Check for the categorical variables in whethers data</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary:\n",
      " ['Clear' 'Foggy' 'Partly Cloudy' 'Mostly Cloudy' 'Breezy and Foggy'\n",
      " 'Overcast' 'Humid and Partly Cloudy' 'Humid and Mostly Cloudy'\n",
      " 'Humid and Overcast' 'Humid and Foggy' 'Humid' 'Possible Light Rain'\n",
      " 'Rain' 'Light Rain' 'Light Rain and Humid'\n",
      " 'Possible Light Rain and Humid' 'Possible Drizzle and Humid'\n",
      " 'Rain and Humid']\n",
      "Icon:\n",
      " ['clear-night' 'fog' 'clear-day' 'partly-cloudy-day' 'partly-cloudy-night'\n",
      " 'wind' 'cloudy' 'rain']\n",
      "precip_type:\n",
      " ['rain' '-9999']\n"
     ]
    }
   ],
   "source": [
    "print('Summary:\\n',df_wether2.summary.unique())\n",
    "print('Icon:\\n',df_wether2.icon.unique())\n",
    "print('precip_type:\\n',df_wether2.precip_type.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4><li>One hot encode the categorical variables</li>\n",
    "    <li>Generate a new feature to get the week of the year</li>\n",
    "    <li>Generate a new feature to specify the time of day as pahar</li>\n",
    "    <li>Drop not required features from the dataset</li>\n",
    "</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wether2=pd.concat([df_wether2,pd.get_dummies(df_wether2.precip_type,prefix='precip_type')],axis=1)\n",
    "df_wether2=pd.concat([df_wether2,pd.get_dummies(df_wether2.summary,prefix='summary')],axis=1)\n",
    "df_wether2=pd.concat([df_wether2,pd.get_dummies(df_wether2.icon,prefix='icon')],axis=1)\n",
    "df_wether2['week_of_year']=df_wether2.datetime_local.dt.weekofyear\n",
    "df_wether2['pahar']=df_wether2.datetime_local.dt.hour.apply(lambda hr: get_pahar(hr))\n",
    "df_wether2.drop(['icon','plant_id','summary','sunrise','sunset','updated_at','precip_type','datetime_utc'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> For using time series algorithms we need to have a continous time series data,<br>\n",
    "    Check if we have Missing Days in the data</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counts for Whether Data\n",
      "Required no. of Days:  730 \n",
      "Existing no. of Days:  568 \n",
      "Missing no. Of Days:  162\n"
     ]
    }
   ],
   "source": [
    "print('Counts for Whether Data')\n",
    "print('Required no. of Days: ',(df_wether2.datetime_local.max()-df_wether2.datetime_local.min()).days+1,\\\n",
    "        '\\nExisting no. of Days: ',len(df_wether2.datetime_local.dt.date.unique()),\\\n",
    "        '\\nMissing no. Of Days: ',(df_wether2.datetime_local.max()-df_wether2.datetime_local.min()).days-len(df_wether2.datetime_local.dt.date.unique())+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counts for Power Data\n",
      "Required no. of Days:  730 \n",
      "Existing no. of Days:  730 \n",
      "Missing no. Of Days:  0\n"
     ]
    }
   ],
   "source": [
    "print('Counts for Power Data')\n",
    "print('Required no. of Days: ',(df_power.datetime.max()-df_power.datetime.min()).days+1,\\\n",
    "        '\\nExisting no. of Days: ',len(df_power.datetime.dt.date.unique()),\\\n",
    "        '\\nMissing no. Of Days: ',(df_power.datetime.max()-df_power.datetime.min()).days-len(df_power.datetime.dt.date.unique())+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Looking at list of missing days we can clearly see that data is missing for dates between 28th of Feb 2018 till 15th Aug 2018.</h4>\n",
    "<p>Imputing data for these many days consecutively will degrade the performance of models.\n",
    "Hence we can ignore this missing dates for now.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataDate count: 7377\n",
      "Actual Date count: 729\n",
      "Missing Date count: 163\n",
      "['28February2018', '01March2018', '02March2018', '03March2018', '04March2018', '05March2018', '06March2018', '13March2018', '14March2018', '15March2018', '16March2018', '17March2018', '18March2018', '19March2018', '20March2018', '21March2018', '22March2018', '23March2018', '24March2018', '25March2018', '26March2018', '27March2018', '28March2018', '29March2018', '30March2018', '31March2018', '01April2018', '02April2018', '03April2018', '04April2018', '05April2018', '06April2018', '07April2018', '08April2018', '09April2018', '10April2018', '11April2018', '12April2018', '13April2018', '14April2018', '15April2018', '16April2018', '17April2018', '18April2018', '19April2018', '20April2018', '21April2018', '22April2018', '23April2018', '24April2018', '25April2018', '26April2018', '27April2018', '28April2018', '29April2018', '30April2018', '01May2018', '02May2018', '03May2018', '04May2018', '05May2018', '06May2018', '07May2018', '08May2018', '09May2018', '10May2018', '11May2018', '12May2018', '13May2018', '14May2018', '15May2018', '16May2018', '17May2018', '18May2018', '19May2018', '20May2018', '21May2018', '22May2018', '23May2018', '24May2018', '25May2018', '26May2018', '27May2018', '28May2018', '29May2018', '30May2018', '31May2018', '01June2018', '02June2018', '03June2018', '04June2018', '05June2018', '06June2018', '07June2018', '08June2018', '09June2018', '10June2018', '11June2018', '12June2018', '13June2018', '14June2018', '15June2018', '16June2018', '17June2018', '18June2018', '19June2018', '20June2018', '21June2018', '22June2018', '23June2018', '24June2018', '25June2018', '26June2018', '27June2018', '28June2018', '29June2018', '30June2018', '01July2018', '02July2018', '03July2018', '04July2018', '05July2018', '06July2018', '07July2018', '08July2018', '09July2018', '10July2018', '11July2018', '12July2018', '13July2018', '14July2018', '15July2018', '16July2018', '17July2018', '18July2018', '19July2018', '20July2018', '21July2018', '22July2018', '23July2018', '24July2018', '25July2018', '26July2018', '27July2018', '28July2018', '29July2018', '30July2018', '31July2018', '01August2018', '02August2018', '03August2018', '04August2018', '05August2018', '06August2018', '07August2018', '08August2018', '09August2018', '10August2018', '11August2018', '12August2018', '13August2018', '14August2018', '15August2018']\n"
     ]
    }
   ],
   "source": [
    "d=df_wether2.sort_values('datetime_local').datetime_local.to_list()\n",
    "print('DataDate count:',len(d))\n",
    "date_set = set(d[0] + timedelta(x) for x in range((d[-1] - d[0]).days))\n",
    "print('Actual Date count:',len(date_set))\n",
    "missing = sorted(date_set - set(d))\n",
    "print('Missing Date count:',len(missing))\n",
    "print([datetime.strftime(dt,\"%d%B%Y\") for dt in missing])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> We will join the Power data and Wether data generating new preprocessed data having all the features for each 15min interval time</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wether2['Date']=df_wether2.datetime_local.dt.strftime('%Y-%m-%d %HH')\n",
    "df_power['Date']=df_power.datetime.dt.strftime('%Y-%m-%d %HH')\n",
    "df_15min_int_data=df_power.join(df_wether2.set_index('Date'),on=['Date'],how='inner').drop(['Date','datetime_local'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Check if combined data is having any NAN Values</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime                                 0\n",
       "power                                    0\n",
       "cloud_cover                              0\n",
       "apparent_temperature                     0\n",
       "temperature                              0\n",
       "humidity                                 0\n",
       "dew_point                                0\n",
       "wind_bearing                             0\n",
       "wind_speed                               0\n",
       "wind_gust                                0\n",
       "pressure                                 0\n",
       "uv_index                                 0\n",
       "ozone                                    0\n",
       "precip_intensity                         0\n",
       "precip_probability                       0\n",
       "visibility                               0\n",
       "precip_type_-9999                        0\n",
       "precip_type_rain                         0\n",
       "summary_Breezy and Foggy                 0\n",
       "summary_Clear                            0\n",
       "summary_Foggy                            0\n",
       "summary_Humid                            0\n",
       "summary_Humid and Foggy                  0\n",
       "summary_Humid and Mostly Cloudy          0\n",
       "summary_Humid and Overcast               0\n",
       "summary_Humid and Partly Cloudy          0\n",
       "summary_Light Rain                       0\n",
       "summary_Light Rain and Humid             0\n",
       "summary_Mostly Cloudy                    0\n",
       "summary_Overcast                         0\n",
       "summary_Partly Cloudy                    0\n",
       "summary_Possible Drizzle and Humid       0\n",
       "summary_Possible Light Rain              0\n",
       "summary_Possible Light Rain and Humid    0\n",
       "summary_Rain                             0\n",
       "summary_Rain and Humid                   0\n",
       "icon_clear-day                           0\n",
       "icon_clear-night                         0\n",
       "icon_cloudy                              0\n",
       "icon_fog                                 0\n",
       "icon_partly-cloudy-day                   0\n",
       "icon_partly-cloudy-night                 0\n",
       "icon_rain                                0\n",
       "icon_wind                                0\n",
       "week_of_year                             0\n",
       "pahar                                    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_15min_int_data.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Save the data frame to a file</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_15min_int_data.to_csv('climate_15min_int_data.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
